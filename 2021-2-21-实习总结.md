## 论文

1. Ruan et al, **Towards** an ASR error robust Spoken Language Understanding System, Amazon Alexa, InterSpeech 2020. 
2. Gopalakrishnan, Are Neural **Open-Domain** Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study, Amazon Alexa, InterSpeech 2020. 
3. Li et al, Improving Spoken Language Understanding by Exploiting ASR **N-best Hypotheses**, Amazon Alexa, ICASSP 2020. 
4. Huang et al, Learning **ASR-robust Contextualized** Embeddings for Spoken Language Understanding, NTU, ICASSP 2020. 
5. Chen et al, SeqVAT:**Virtual Adversarial Training** for Semi-Supervised Sequence Labeling, Amazon Alexa, ACL 2020. 
6. Fang et al, **Using Phoneme Representations** to Build Predictive Models Robust to ASR Errors, Amazon Alexa, SIGIR 2020. 
7. Li et al, **Multi-task** Learning of Spoken Language Understanding by Integrating N-Best Hypotheses with Hierarchical Attention, Amazon Alexa, COLING 2020. 
8. Short Text Classification using **Graph Convolutional Network** ,Amazon, NeurIPS 2019.
9. PHONEME-BERT: JOINT LANGUAGE MODELLING OF PHONEME SEQUENCE AND ASR TRANSCRIPT

## 鲁棒性优化方向
- 数据输入： nbest; lattice; add more feature(am lm phoneme)
- 预训练: nlp的预训练数据通常是标准的写作文本，考虑到asr噪声，通过修改预训练模型和输入数据以增强embedding表征（phoneme-bert）
- 模型方面： CNN, LSTM, Bert, GCN
- 对抗训练： kl loss
- 多任务


## 实验部分

### 1.Bert外呼实验(其后为相关目录)：
结合论文IMPROVING SLU BY EXPLOITING ASR N-BEST HYPOTHESES，在文本层面对ASR输出的top-n结果进行拼接预测实验
- Bert top1分类(Top1_Classify)
- Bert top-N分类得出N=4时候预测效果和速度较好的平衡(Concatenate_Classify)
- Bert top-N+ am,lm (Concatenate_Classify_MoreFeature)
- Bert top-N+ am,lm 分桶实验，在Bert embedding层面，对各个拼接的句子embedding使用am,lm加权进行预测，效果得到一定提升，线上就采用的此版本(Concatenate_Classify_MoreFeature_weighting)

### 2.鲁棒性论文研究

复现和实验论文：

- chen yun nung (ASR-ROBUST CONTEXTUALIZED EMBEDDINGS)论文在预训练端，通过增加asr-trans文本中对应位置单词的相似度loss，反向优化时候拉齐相似单词的embedding表征，来增强预训练层面的embedding表征，实验复现效果一般。不过预训练端增强embedding表征是一个较新颖的研究方向（spokenvec/）

- Amazon(Towards an ASR error robust SLU)论文提出了鲁棒性研究应该在对ASR文本预测正确率提升的同时，对原Trans文本预测精度不能下降的观点，在预测端通过同时预测ASR和Trans文本的分布，增加了KL loss，实现了论文的观点，并且预测精度得到一定的提升。（KLloss/ ）

- 拼接音素实验：由于数据量的原因，效果并不明显。（classification_CNN_phoneme/）

研读论文：

- lattice RNN for SLU: 特定数据输入信息更为丰富，实验结果提升很多，但是缺点是速度非常慢，因为在进行batch处理的时候，每个time step都有最耗时的操作，整体下来速度慢了很多。
- lattice Transformer：在transformer中，通过修改输入层seq tag为对应位置可能存在的多个字，从而达到引入lattice信息目的，实验速度相对latticeRNN有一定提升，效果没有同比
- Using Phoneme Representations: 考虑到语音识别音素较识别后的单词对应的per较wer更低，拼接低维的音素embedding对意图分类预测精度得到一部分提升。
- Multi-task Learning of SLU by Integrating N-Best Hypotheses with Hierarchical Attention： 
- Data balancing for boosting performance of low-frequency classes in SLU：数据不均衡的情况下，对比了带权loss，重采样，多任务batch数据类别平衡生成器等方法进行组合实验，证明了加入多任务类别平衡的生成器，在不降低主要类别识别精度的情况下，提升了长尾意图的识别精度
- Confusion2Vec：借鉴word2vec的思想，与增强预训练表征思想一致，文章篇幅过长。
- Phoneme-Bert: 加入音素信息，使用Bert结构，A句为英文sentence，B句为音素sentence，位置编码二者都是从0开始一一对应，较好的利用到了音素的信息。

### 3.相关实验CF链接
- 鲁棒性NLU预研：https://cf.jd.com/pages/viewpage.action?pageId=422663628
- 鲁棒性加入音素特征：https://cf.jd.com/pages/viewpage.action?pageId=406413000
- 大件外呼N-best实验：https://cf.jd.com/pages/viewpage.action?pageId=374479858


在京东实习的三个月时间里，参与并负责了外呼Bert模型实验，包括
- top1-Bert, 
- n-best-bert, 
- n-best-bert_with_am_lm,
- n-best-bert_with_am_lm_bucket

统计分析并绘制了实验数据结果，进一步规范了个人实验研究方法。

在鲁棒性研究方面，学习研究：

- 数据输入： nbest; lattice; add more feature(am lm phoneme)
- 预训练: nlp的预训练数据通常是标准的写作文本，考虑到asr噪声，通过修改预训练模型和输入数据以增强embedding表征（phoneme-bert， contextual robust）
    - LEARNING ASR-ROBUST CONTEXTUALIZED EMBEDDINGS FOR SPOKEN LANGUAGE UNDERSTANDING
    - Learning Spoken Language Representations with Neural Lattice Language Modeling
    - ADAPTING PRETRAINED TRANSFORMER TO LATTICES FOR SPOKEN LANGUAGE UNDERSTANDING
    - Spoken Language Intent Detection using Confusion2Vec
    - PHONEME-BERT
- 模型方面： CNN, LSTM, Bert, GCN
- 对抗训练： 从任务预测分类的分布入手，通过拉齐asr-trans的预测分布优化权重参数
    - Towards an ASR error robust Spoken Language Understanding System
- 多任务： 解决数据不平衡问题；增加领域分类，翻译重建提升意图分类准确率
    - Data balancing for boosting performance of low-frequency classes in Spoken Language Understanding
    - Multi-task Learning of Spoken Language Understanding by Integrating N-Best Hypotheses with Hierarchical Attention
- 端到端： 未涉及.

SLU系统的鲁棒性研究是一个具有潜力和价值的研究方向，在三个月的实习期里，从零到现在对鲁棒性研究有了一个较全面的认识这让我受益匪浅。

## 端到端SLU
<TOWARDS END-TO-END SPOKEN LANGUAGE UNDERSTANDING> -Bengio et. al. 2018

end2end examples in asr:

- the CTC loss function is used to train an ASR system to map the feature sequences directly to the word sequences in [16] and it has been shown to perform similarly to the traditional ASR systems 
    - [16] “Neural speech rec- ognizer: acoustic-to-word LSTM model for large vocabulary speech recognition,” 
- in [17, 18], encode-decoder models with attention have been used for ASR tasks 
    - [17] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in NIPS, 2015.
    - [18] Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk, Phile- mon Brakel, and Yoshua Bengio, “End-to-end attention-based large vocabulary speech recognition
- End-of-end learning of memory networks is also used for knowledge carryover in multi-turn spoken language un- derstanding [19]
    - “End-to-end memory networks with knowl- edge carryover for multi-turn spoken language understanding,” in Interspeech, 2016.

in this work, we focus on maximizing the single-turn intent classification accuracy **using log-Mel filterbank feature** directly.

CD-HMM-LSTM [20] is widely used as an AM, in which the feature vector sequence is converted to the likelihood vectors of context-dependent HMM states for each acoustic frame
    - [20] Has¸im Sak, Andrew Senior, “Long short-term memory recurrent neural network architectures for large scale acoustic modeling,” in Interspeech, 2014.

## ASR ——>NLU ——> SLU 三部分是分开训练
In the pipelined approach to SLU, ASR, and NLU modules are usually trained independently, where the NLU module is trained using human transcription as the input. During the evaluation phase, the ASR output is piped into the NLU module

## 每10ms取log-Mel特征 存在一定问题
- A potential issue of using log-Mel filterbank feature is that it is generated every 10 ms: the 10-ms frame rate is good for classifying sub-phone unit like CD-HMM states, while it may not be suitable for utterance-level classification as GRUs may forget most of the speech content when it arrives at the end of the utterance end due to gradient vanishing. 
- In order to reduce the sequence length processed by GRUs, we subsample [10, 15] the hidden activations along the time domain for every bi-direction GRU layer (see Fig. 4). **This allowed us to extract a representation roughly at a syllable level in a given utterance**. On the other hand, this significantly reduced the computational time for both training and prediction, which enables us to use bi-directional GRU for real time intent and/or domain classification. Given the encoder output, a max-pooling layer along the time axis is used to compress it into a fixed dimension vector. This is followed by a fully- connected feed-forward layer. Finally, a softmax layer is used to compute the posterior probability of intents or domains.

