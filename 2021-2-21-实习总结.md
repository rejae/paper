## 论文

1. Ruan et al, **Towards** an ASR error robust Spoken Language Understanding System, Amazon Alexa, InterSpeech 2020. 
2. Gopalakrishnan, Are Neural **Open-Domain** Dialog Systems Robust to Speech Recognition Errors in the Dialog History? An Empirical Study, Amazon Alexa, InterSpeech 2020. 
3. Li et al, Improving Spoken Language Understanding by Exploiting ASR **N-best Hypotheses**, Amazon Alexa, ICASSP 2020. 
4. Huang et al, Learning **ASR-robust Contextualized** Embeddings for Spoken Language Understanding, NTU, ICASSP 2020. 
5. Chen et al, SeqVAT:**Virtual Adversarial Training** for Semi-Supervised Sequence Labeling, Amazon Alexa, ACL 2020. 
6. Fang et al, Using **Phoneme Representations** to Build Predictive Models Robust to ASR Errors, Amazon Alexa, SIGIR 2020. 
7. Li et al, **Multi-task** Learning of Spoken Language Understanding by Integrating N-Best Hypotheses with Hierarchical Attention, Amazon Alexa, COLING 2020. 
8. Short Text Classification using **Graph Convolutional Network** ,Amazon, NeurIPS 2019.



## 实验部分

### 1.Bert外呼实验(其后为相关目录)：
结合论文IMPROVING SLU BY EXPLOITING ASR N-BEST HYPOTHESES，在文本层面对ASR输出的top-n结果进行拼接预测实验
- Bert top1分类(Top1_Classify)
- Bert top-N分类得出N=4时候预测效果和速度较好的平衡(Concatenate_Classify)
- Bert top-N+ am,lm (Concatenate_Classify_MoreFeature)
- Bert top-N+ am,lm 分桶实验，在Bert embedding层面，对各个拼接的句子embedding使用am,lm加权进行预测，效果得到一定提升，线上就采用的此版本(Concatenate_Classify_MoreFeature_weighting)

### 2.鲁棒性论文研究

复现和实验论文：

- chen yun nung （ASR-ROBUST CONTEXTUALIZED EMBEDDINGS）论文在预训练端，通过增加asr-trans文本中对应位置单词的相似度loss，反向优化时候拉齐相似单词的embedding表征，来增强预训练层面的embedding表征，实验复现效果一般。不过预训练端增强embedding表征是一个较新颖的研究方向（spokenvec/）

- Amazon(Towards an ASR error robust SLU)论文提出了鲁棒性研究应该在对ASR文本预测正确率提升的同时，对原Trans文本预测精度不能下降的观点，在预测端通过同时预测ASR和Trans文本的分布，增加了KL loss，实现了论文的观点，并且预测精度得到一定的提升。（KLloss/ ）

- 拼接音素实验：由于数据量的原因，效果并不明显。（classification_CNN_phoneme/）

研读论文：

- lattice RNN for SLU: 特定数据输入信息更为丰富，实验结果提升很多，但是缺点是速度非常慢，因为在进行batch处理的时候，每个time step都有最耗时的操作，整体下来速度慢了很多。
- lattice Transformer：在transformer中，通过修改输入层seq tag为对应位置可能存在的多个字，从而达到引入lattice信息目的，实验速度相对latticeRNN有一定提升，效果没有同比
- add phoneme embedding: 考虑到语音识别音素较识别后的单词对应的per较wer更低，拼接低维的音素embedding对意图分类预测精度得到一部分提升。
- Multi-task Learning of SLU by Integrating N-Best Hypotheses with Hierarchical Attention： 
- Data balancing for boosting performance of low-frequency classes in SLU：数据不均衡的情况下，对比了带权loss，重采样，多任务batch数据类别平衡生成器等方法进行组合实验，证明了加入多任务类别平衡的生成器，在不降低主要类别识别精度的情况下，提升了长尾意图的识别精度
- Confusion2Vec：借鉴word2vec的思想，与增强预训练表征思想一致，文章篇幅过长。

### 3.相关实验CF链接
- 鲁棒性NLU预研：https://cf.jd.com/pages/viewpage.action?pageId=422663628
- 鲁棒性加入音素特征：https://cf.jd.com/pages/viewpage.action?pageId=406413000
- 大件外呼N-best实验：https://cf.jd.com/pages/viewpage.action?pageId=374479858